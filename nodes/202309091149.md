---
tags:
  - compsci/AI/ML
---
## question
What is batch gradient descent learning?
## evidence
- Gradient descent is an approach to reducing the empirical risk function [^1] through means of forward and backward propagation [^2].
- In neural networks, the model minimizes the risk function by backpedaling along its initial neural path in an attempt to find an alternative route; however, this is in the context of single data inputs.
- Batch gradient descent learning involves forward/backward propagation with a *batch* of input data.

[^1]: [[202309050151]]
[^2]: what is forward/backward propagation?
## conclusion
Conceptually, **Batch Gradient Descent** optimizes the empirical risk function [^1] through the process of forward and backward propagation using a batch of input data. In other words, the algorithm backpedals, when given a large pool of input data, in an attempt to minimize the empirical risk function [^1].